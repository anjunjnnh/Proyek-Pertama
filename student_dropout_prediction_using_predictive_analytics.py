# -*- coding: utf-8 -*-
"""Student Dropout Prediction Using Predictive Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OK3lXUqYLJLO2BV113Omp-uGwB6p22OY

# **Student Dropout Prediction Using Predictive Analytics**

---

oleh : Anju Anjannah

## **Deskripsi Proyek**
Proyek ini bertujuan untuk mengembangkan model machine learning yang dapat memprediksi status dropout mahasiswa dengan lebih akurat dan efisien. Tingginya angka dropout mahasiswa merupakan permasalahan signifikan di institusi pendidikan. Proses identifikasi mahasiswa yang berisiko dropout secara manual seringkali sulit, memakan waktu, dan tidak akurat, yang dapat menyebabkan terlambatnya intervensi dan dukungan yang dibutuhkan mahasiswa. Hal ini dapat berdampak negatif pada mahasiswa itu sendiri maupun performa institusi. Model prediksi dropout mahasiswa dapat membantu mengatasi permasalahan ini dengan memberikan solusi yang lebih akurat, efisien, dan berbasis data, memungkinkan institusi untuk mengidentifikasi dan memberikan dukungan tepat waktu kepada mahasiswa yang membutuhkan.

# **1. Import Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression,Perceptron
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay

"""# **2. Data Understanding**

**2.1 Data loading**

Sumber Dataset : https://www.kaggle.com/datasets/adilshamim8/predict-students-dropout-and-academic-success/code
"""

from google.colab import files
uploaded = files.upload()

filename = list(uploaded.keys())[0]
data = pd.read_csv(filename)

data

"""**2.2 EDA**"""

data.info()

data.isnull().sum()

data.describe().T

data['target'].value_counts()

data['target'] = LabelEncoder().fit_transform(data['target'])

data['target'].value_counts()

plt.figure(figsize=(5, 10))
sns.histplot(data['target'], color="Blue", kde=True)
plt.show()

plt.figure(figsize=(5, 10))
sns.countplot(data = data, x="target").set_title('Target Distribution')
plt.show()

plt.figure(figsize=(8, 8))
plt.title("Education Status")
plt.pie(data['target'].value_counts(), labels = ['Graduate', 'Dropout', 'Enrolled'], explode = (0.0, 0.0, 0.0), autopct='%1.2f%%', shadow = True)
plt.legend( loc = 'lower right')
plt.show()

plt.figure(figsize=(8, 8))
plt.title("Gender")
plt.pie(data['Gender'].value_counts(), labels = ['Male', 'Female'], explode = (0.1, 0.0), autopct='%1.2f%%', shadow = True)
plt.legend( loc = 'lower right')
plt.show()

plt.figure(figsize=(20, 45))

for i in range(0, 35):
    plt.subplot(12, 3, i+1)
    sns.histplot(data.iloc[:, i], color='blue', kde=True)
    plt.title(data.columns[i])
    plt.grid()

plt.tight_layout()
plt.show()

#feature selection
corr_matrix = data.corr(method="pearson")
plt.figure(figsize=(10, 10))
sns.heatmap(corr_matrix, vmin=-1., vmax=1., annot=False, fmt='.2f', cmap="YlGnBu", cbar=True, linewidths=0.5)
plt.title("Feature Correlation Matrix")
plt.show()

["Tuition fees up to date","Curricular units 1st sem (approved)","Curricular units 1st sem (grade)","Curricular units 2nd sem (approved)","Curricular units 2nd sem (grade)"]
corr_matrix["target"]

"""# **3. Data Preparation**

**3.1 Data Cleaning**
"""

data.drop(data[data['target'] == 1].index, inplace = True)
data

data['Dropout'] = data['target'].apply(lambda x: 1 if x==0 else 0)
data

plt.figure(figsize=(5, 10))
sns.histplot(data['Dropout'], color="red", kde=True) # Use histplot and add kde=True for a kernel density estimate line
plt.show()

plt.figure(figsize=(8, 8))
plt.title("Dropout Status")
plt.pie(data['Dropout'].value_counts(),  labels = ['Non-Dropout', 'Dropout'], explode = (0.1, 0.0), autopct='%1.2f%%', shadow = False)
plt.legend( loc = 'lower right')
plt.show()

"""**3.2 Standarisasi**"""

x = data.iloc[:, :36].values
#x = df[["Tuition fees up to date","Curricular units 1st sem (approved)","Curricular units 1st sem (grade)","Curricular units 2nd sem (approved)","Curricular units 2nd sem (grade)"]].values
print(x)
x = StandardScaler().fit_transform(x)
x

y = data['Dropout'].values
y

"""**3.3 Split Data**"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)

"""# **4. Model Development dan Evaluation**"""

def perform(y_pred):
    print("Precision : ", precision_score(y_test, y_pred, average = 'micro'))
    print("Recall : ", recall_score(y_test, y_pred, average = 'micro'))
    print("Accuracy : ", accuracy_score(y_test, y_pred))
    print("F1 Score : ", f1_score(y_test, y_pred, average = 'micro'))
    cm = confusion_matrix(y_test, y_pred)
    print("\n", cm)
    print("\n")
    print("**"*27 + "\n" + " "* 16 + "Classification Report\n" + "**"*27)
    print(classification_report(y_test, y_pred))
    print("**"*27+"\n")

    cm = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=['Non-Dropout', 'Dropout'])
    cm.plot()

"""a. Gaussian Naive Bayes"""

model1_nb = GaussianNB()
model1_nb.fit(x_train, y_train)

y_pred_model1 = model1_nb.predict(x_test)

perform(y_pred_model1)

"""b. Logistic Regression"""

model2_lr = LogisticRegression()
model2_lr.fit(x_train, y_train)

y_pred_model2 = model2_lr.predict(x_test)

perform(y_pred_model2)

"""c. Random Forest Classifier"""

model3_rf = RandomForestClassifier(n_estimators=500,criterion='entropy')
model3_rf.fit(x_train, y_train)

y_pred_model3 = model3_rf.predict(x_test)

perform(y_pred_model3)

"""# **5. Comparison of Results**"""

predict=[y_pred_model1,y_pred_model2,y_pred_model3]
acc=[]
classifiers=["NaiveBayes","Logistic Regression","RandomForest"]
for i in predict:
    temp=accuracy_score(y_test, i)
    acc.append(temp)

# Temukan indeks akurasi terbaik
best_acc_index = np.argmax(acc)

# Buat daftar warna
colors = ['blue'] * len(classifiers)
colors[best_acc_index] = 'red'
plt.barh(classifiers, acc, color=colors)

# Add labels and title
plt.ylabel('Classifiers')
plt.xlabel('Accuracy')
plt.title('Perbandingan Accuracy Model')
plt.show()